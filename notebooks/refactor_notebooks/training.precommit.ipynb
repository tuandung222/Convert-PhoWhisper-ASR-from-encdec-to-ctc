{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This notebook is used for training the model\n",
    "- Please refer to my notebook named \"evaluate_after_training\" for:\n",
    "    - Loading the best checkpoint from my Huggingface repository (I have pushed to it after training)\n",
    "    - Evaluating the model on the test set\n",
    "    - Visualizing the results (if you want to hear some audio from the test set and see the predicted transcription from the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Datamodule, ModelModule, and Eval Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets transformers librosa soundfile jiwer evaluate --quiet\n",
    "# %pip install pytorch_lightning torch bitsandbytes --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from bitsandbytes.optim import Adam8bit\n",
    "from datasets import load_dataset\n",
    "from easydict import EasyDict as edict\n",
    "from evaluate import load as load_metric\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    AutoProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from transformers.models.whisper.modeling_whisper import WhisperEncoder\n",
    "\n",
    "\n",
    "class VietBud500DataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 32,\n",
    "        processor_name: str = \"vinai/PhoWhisper-tiny\",\n",
    "        num_workers: int = 2,\n",
    "        pin_memory: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.processor = AutoProcessor.from_pretrained(processor_name)\n",
    "\n",
    "        print(\"Download just 3 shards / 105 shards of the origin training data\")\n",
    "        self.train_url = [\n",
    "            \"https://huggingface.co/datasets/linhtran92/viet_bud500/resolve/main/data/train-00000-of-00105-be5f872f8be772f5.parquet\",\n",
    "            \"https://huggingface.co/datasets/linhtran92/viet_bud500/resolve/main/data/train-00097-of-00105-4160c0470220c086.parquet\",\n",
    "            \"https://huggingface.co/datasets/linhtran92/viet_bud500/resolve/main/data/train-00086-of-00105-131a0bbf617d895c.parquet\",\n",
    "        ]\n",
    "        self.test_url = \"https://huggingface.co/datasets/linhtran92/viet_bud500/resolve/main/data/test-00000-of-00002-531c1d81edb57297.parquet\"\n",
    "        self.data_files = {\"train\": self.train_url, \"test\": self.test_url}\n",
    "\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.dataset = load_dataset(\n",
    "            \"parquet\",\n",
    "            data_files=self.data_files,\n",
    "        )\n",
    "        self.sampling_rate = self.dataset[\"train\"].features[\"audio\"].sampling_rate\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        test_dataset = self.dataset[\"test\"]\n",
    "\n",
    "        train_dataset = self.dataset[\"train\"].shuffle(seed=42)\n",
    "        train_val_split = train_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "        self.train_dataset = train_val_split[\"train\"]\n",
    "        self.val_dataset = train_val_split[\"test\"]\n",
    "\n",
    "        print(\n",
    "            \"Just select 1000 examples from a shard of the origin test data serving as the test split!\"\n",
    "        )\n",
    "        self.test_dataset = test_dataset.select(range(1000))\n",
    "\n",
    "        print(\"Number of training examples:\", len(self.train_dataset))\n",
    "        print(\"Number of validation examples:\", len(self.val_dataset))\n",
    "        print(\"Number of test examples:\", len(self.test_dataset))\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # Extract audio and transcription from the batch\n",
    "        audios = [item[\"audio\"][\"array\"] for item in batch]\n",
    "        transcriptions = [item[\"transcription\"] for item in batch]\n",
    "\n",
    "        # Process audio and transcription using the processor\n",
    "        inputs = self.processor(\n",
    "            audios,\n",
    "            sampling_rate=self.sampling_rate,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Tokenize transcriptions\n",
    "        # with self.processor.as_target_processor(): # prepared correctly without the decoder's bos token\n",
    "        # if True:\n",
    "        labels = self.processor(\n",
    "            text=transcriptions,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "        ).input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_features\": inputs.input_features,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.collate_fn,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoWhisperLightningModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/PhoWhisper-tiny\",\n",
    "        learning_rate: float = 5e-5,\n",
    "        warmup_steps: int = 1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()  # Save hyperparameters for logging\n",
    "        self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "        # self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "        temp_model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.encoder = WhisperEncoder(config=self.config)\n",
    "        self.encoder.load_state_dict(temp_model.model.encoder.state_dict(), strict=True)\n",
    "        del temp_model\n",
    "\n",
    "        self.ctc_head = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, self.config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(self.config.hidden_size),\n",
    "            nn.Linear(self.config.hidden_size, self.processor.tokenizer.vocab_size),\n",
    "        )\n",
    "\n",
    "        self.ctc_loss = torch.nn.CTCLoss(\n",
    "            blank=self.processor.tokenizer.pad_token_id, zero_infinity=True\n",
    "        )\n",
    "\n",
    "        # Hyperparameters for AdamW optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def forward(self, input_features, labels=None):\n",
    "        encoder_outputs = self.encoder(input_features)  # (batch, time, hidden)\n",
    "        logits = self.ctc_head(encoder_outputs.last_hidden_state)  # (batch, time, vocab)\n",
    "        logits = logits.transpose(0, 1)  # (time, batch, vocab)\n",
    "\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=2)\n",
    "        input_lengths = torch.full(\n",
    "            size=(log_probs.size(1),),\n",
    "            fill_value=log_probs.size(0),\n",
    "            dtype=torch.int32,\n",
    "        )\n",
    "        if labels is not None:\n",
    "\n",
    "            # replace first bos token by pad token (blank token)\n",
    "            labels[labels == self.processor.tokenizer.bos_token_id] = (\n",
    "                self.processor.tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            label_mask = labels != self.processor.tokenizer.pad_token_id\n",
    "            labels = labels[label_mask].to(torch.int32)\n",
    "            label_lengths = label_mask.sum(dim=1)\n",
    "            assert label_lengths.sum() == labels.size(\n",
    "                0\n",
    "            )  # \"Sum of label_lengths must equal number of labels.\"\n",
    "\n",
    "            loss = self.ctc_loss(log_probs, labels, input_lengths, label_lengths)\n",
    "            self.log(\n",
    "                \"train_loss\",\n",
    "                loss,\n",
    "                on_step=True,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "            return edict(\n",
    "                {\n",
    "                    \"loss\": loss,\n",
    "                    \"logits\": logits if labels is not None else None,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            return edict({\"logits\": logits})\n",
    "\n",
    "    def seq2seq_forward(self, input_features, labels=None):\n",
    "        # Seq2Seq forward pass\n",
    "        # Current not used\n",
    "        return self.model(input_features=input_features, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_features = batch[\"input_features\"]\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = self(input_features=input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_features = batch[\"input_features\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = self(input_features=input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return outputs\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_features = batch[\"input_features\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = self(input_features=input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return outputs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=0.1,\n",
    "            betas=(0.9, 0.98),\n",
    "            eps=1e-6,\n",
    "        )\n",
    "        # optimizer = Adam8bit(self.parameters(), lr=self.learning_rate, eps=1e-8)\n",
    "        train_dataloader = self.trainer.datamodule.train_dataloader()\n",
    "        total_steps = len(train_dataloader) * self.trainer.max_epochs\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=total_steps,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        # Save the processor along with the model checkpoint\n",
    "        checkpoint[\"processor\"] = self.processor\n",
    "\n",
    "    def on_load_checkpoint(self, checkpoint):\n",
    "        self.processor = checkpoint[\"processor\"]\n",
    "\n",
    "    def ctc_decode(self, logits, processor=None):\n",
    "        if processor is None:\n",
    "            processor = self.processor\n",
    "        # logits shape: (time, batch, vocab)\n",
    "        logits = logits.transpose(0, 1)  # (batch, time, vocab)\n",
    "        class_indices = logits.argmax(dim=2)\n",
    "        texts = []\n",
    "        for seq in class_indices:\n",
    "            # Remove blanks (pad tokens)\n",
    "            seq_no_blank = seq[seq != processor.tokenizer.pad_token_id]\n",
    "            # Collapse repeats\n",
    "            seq_collapsed = []\n",
    "            prev_token = -1\n",
    "            for token in seq_no_blank:\n",
    "                if token != prev_token:\n",
    "                    seq_collapsed.append(token.item())\n",
    "                    prev_token = token\n",
    "            # Decode to text\n",
    "            text = processor.decode(seq_collapsed, skip_special_tokens=False)\n",
    "            texts.append(text)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer_evaluate(pl_module, test_dataloader, device=\"cuda\"):\n",
    "    # This legacy function is uses for Seq2Seq model, currently not used\n",
    "    # Load the WER metric\n",
    "    wer_metric = load_metric(\"wer\")\n",
    "    # Initialize lists to hold predictions and references\n",
    "    predictions = []\n",
    "    references = []\n",
    "    pl_module.to(device)\n",
    "    # Set the model to evaluation mode\n",
    "    pl_module.eval()\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=True):\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_dataloader):\n",
    "                # Move input features and labels to the correct device\n",
    "                input_features = batch[\"input_features\"].to(pl_module.device)\n",
    "                labels = batch[\"labels\"].to(pl_module.device)\n",
    "\n",
    "                # Generate outputs\n",
    "                outputs = pl_module.model.generate(input_features=input_features, do_sample=True)\n",
    "                # Decode generated outputs to text\n",
    "                predicted_texts = datamodule.processor.batch_decode(\n",
    "                    outputs, skip_special_tokens=True\n",
    "                )\n",
    "                # Handle labels: replace -100 with pad_token_id and decode\n",
    "                labels_cpu = labels.detach().cpu()\n",
    "                label_texts = datamodule.processor.batch_decode(\n",
    "                    labels_cpu, skip_special_tokens=True\n",
    "                )\n",
    "                # Collect predictions and references\n",
    "                predictions.extend(predicted_texts)\n",
    "                references.extend(label_texts)\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    # Return the results as a dictionary\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "\n",
    "def wer_ctc_evaluate(pl_module, test_dataloader, device=\"cuda\"):\n",
    "    wer_metric = load_metric(\"wer\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    pl_module.to(device)\n",
    "    pl_module.eval()\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=True):\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_dataloader):\n",
    "                input_features = batch[\"input_features\"].to(pl_module.device)\n",
    "                labels = batch[\"labels\"].to(pl_module.device)\n",
    "                logits = pl_module(input_features=input_features, labels=None).logits\n",
    "\n",
    "                predicted_texts = pl_module.ctc_decode(logits)\n",
    "                label_texts = pl_module.processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "                predictions.extend(predicted_texts)\n",
    "                references.extend(label_texts)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    print(\"First 5 predictions: \", predictions[:5])\n",
    "    print(\"First 5 references: \", references[:5])\n",
    "    print(\"WER:\", wer)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "\n",
    "class EvalCallback(pl.Callback):\n",
    "    def __init__(self, processor):\n",
    "        super().__init__()\n",
    "        self.processor = processor\n",
    "        self.val_predicted_texts = []\n",
    "        self.val_reference_texts = []\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        # Collect predicted texts and reference texts from the validation batch\n",
    "        logits = outputs.logits.detach().cpu()\n",
    "        labels = batch[\"labels\"].detach().cpu()\n",
    "        # Decode logits to predicted texts\n",
    "        predicted_texts = self.ctc_decode(logits, self.processor)\n",
    "        # Decode labels to reference texts\n",
    "        reference_texts = self.processor.batch_decode(labels, skip_special_tokens=True)\n",
    "        # Collect them\n",
    "        self.val_predicted_texts.extend(predicted_texts)\n",
    "        self.val_reference_texts.extend(reference_texts)\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # Compute WER\n",
    "        wer_metric = load_metric(\"wer\")\n",
    "        wer = wer_metric.compute(\n",
    "            predictions=self.val_predicted_texts, references=self.val_reference_texts\n",
    "        )\n",
    "        # Log the WER\n",
    "        pl_module.log(\"val_wer\", wer, prog_bar=True, logger=True)\n",
    "        print(\"WER on validate data:\", wer)\n",
    "        print(\"First 5 predictions: \", self.val_predicted_texts[:5])\n",
    "        print(\"First 5 references: \", self.val_reference_texts[:5])\n",
    "\n",
    "        # Clear the lists for the next epoch\n",
    "        self.val_predicted_texts = []\n",
    "        self.val_reference_texts = []\n",
    "\n",
    "    def ctc_decode(self, logits, processor):\n",
    "        # logits shape: (time, batch, vocab)\n",
    "        # Transpose to (batch, time, vocab)\n",
    "        logits = logits.transpose(0, 1)\n",
    "        # Get the class indices\n",
    "        class_indices = logits.argmax(dim=2)\n",
    "        # Remove blanks and collapse repeats for each sequence\n",
    "        texts = []\n",
    "        for seq in class_indices:\n",
    "            # Remove blanks (pad tokens)\n",
    "            seq_no_blank = seq[seq != processor.tokenizer.pad_token_id]\n",
    "            # Collapse repeats\n",
    "            seq_collapsed = []\n",
    "            prev_token = -1\n",
    "            for token in seq_no_blank:\n",
    "                if token != prev_token:\n",
    "                    seq_collapsed.append(token.item())\n",
    "                    prev_token = token\n",
    "            # Decode to text\n",
    "            text = processor.decode(seq_collapsed, skip_special_tokens=False)\n",
    "            texts.append(text)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data module\n",
    "datamodule = VietBud500DataModule(batch_size=24, processor_name=\"vinai/PhoWhisper-tiny\")\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "\n",
    "\n",
    "# Initialize the Lightning module\n",
    "lightning_module = PhoWhisperLightningModule(\n",
    "    model_name=\"vinai/PhoWhisper-tiny\", learning_rate=1e-4, warmup_steps=20\n",
    ")\n",
    "\n",
    "# print(\"Evaluate before training\", wer_ctc_evaluate(model, datamodule.test_dataloader()))\n",
    "\n",
    "\n",
    "# Initialize the custom callback\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_wer\", mode=\"min\", save_top_k=1, filename=\"best-{val_wer:.4f}\"\n",
    ")\n",
    "eval_callback = EvalCallback(processor=datamodule.processor)\n",
    "\n",
    "# Initialize the PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    # max_steps=20000,\n",
    "    max_epochs=64,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=True,\n",
    "    precision=\"bf16-mixed\",\n",
    "    callbacks=[lr_monitor, checkpoint_callback, eval_callback],\n",
    "    # accumulate_grad_batches=1,\n",
    ")\n",
    "\n",
    "# Train the model on the training set\n",
    "trainer.fit(lightning_module, datamodule=datamodule)\n",
    "\n",
    "# Test the model on the test set\n",
    "trainer.test(lightning_module, datamodule=datamodule, ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tbps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
